{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Resume Ranker - Dataset Evaluation Notebook\n",
    "\n",
    "Notebook n√†y ƒë·ªçc folder ch·ª©a c√°c file PDF/DOC, g·ªçi model c·ªßa web app ƒë·ªÉ ph√¢n t√≠ch v√† ƒë√°nh gi√° hi·ªáu su·∫•t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Th√™m path ƒë·ªÉ import modules\n",
    "sys.path.append('.')\n",
    "\n",
    "print(\"‚úÖ Import libraries completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c modules t·ª´ web app\n",
    "try:\n",
    "    from services import (\n",
    "        extract_text,\n",
    "        extract_entities_ner,\n",
    "        extract_basic_entities,\n",
    "        extract_skills_spacy,\n",
    "        calculate_semantic_similarity,\n",
    "        nlp,\n",
    "        sbert_model\n",
    "    )\n",
    "    print(\"‚úÖ Services imported successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error importing services: {e}\")\n",
    "\n",
    "try:\n",
    "    from control.uploads_controller import extract_cv_data\n",
    "    print(\"‚úÖ Uploads controller imported successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error importing uploads controller: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n dataset\n",
    "dataset_path = input(\"Enter path to dataset folder: \").strip()\n",
    "if not dataset_path:\n",
    "    dataset_path = \"./dataset\"  # Default path\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"‚ùå Dataset folder not found: {dataset_path}\")\n",
    "    print(\"Please create the folder and add your PDF/DOC files\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dataset folder found: {dataset_path}\")\n",
    "\n",
    "# Li·ªát k√™ c√°c file trong dataset\n",
    "def list_dataset_files(folder_path: str) -> List[str]:\n",
    "    \"\"\"Li·ªát k√™ t·∫•t c·∫£ file PDF v√† DOC trong folder\"\"\"\n",
    "    files = []\n",
    "    supported_extensions = ['.pdf', '.docx', '.doc']\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            _, ext = os.path.splitext(filename.lower())\n",
    "            if ext in supported_extensions:\n",
    "                files.append(file_path)\n",
    "    \n",
    "    return files\n",
    "\n",
    "dataset_files = list_dataset_files(dataset_path)\n",
    "print(f\"\\n   Found {len(dataset_files)} files in dataset:\")\n",
    "for i, file_path in enumerate(dataset_files, 1):\n",
    "    filename = os.path.basename(file_path)\n",
    "    file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "    print(f\"  {i}. {filename} ({file_size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_files(file_paths: List[str]) -> List[Dict]:\n",
    "    \"\"\"X·ª≠ l√Ω t·∫•t c·∫£ file trong dataset\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    print(f\"\\nüîÑ Processing {len(file_paths)} files...\")\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths, 1):\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"\\nüìÑ Processing {i}/{len(file_paths)}: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract text t·ª´ file\n",
    "            text = extract_text(file_path)\n",
    "            \n",
    "            if not text.strip():\n",
    "                print(f\"  ‚ö†Ô∏è No text extracted from {filename}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  ‚úÖ Extracted {len(text)} characters\")\n",
    "            \n",
    "            # Extract entities using web app functions\n",
    "            entities = extract_entities_ner(text)\n",
    "            emails, phones, years_exp = extract_basic_entities(text)\n",
    "            skills = extract_skills_spacy(text)\n",
    "            \n",
    "            # T·∫°o CV data structure\n",
    "            cv_data = {\n",
    "                'filename': filename,\n",
    "                'file_path': file_path,\n",
    "                'text': text,\n",
    "                'extracted_data': {\n",
    "                    'name': entities['PERSON'][0] if entities['PERSON'] else \"\",\n",
    "                    'email': emails[0] if emails else \"\",\n",
    "                    'phone': phones[0] if phones else \"\",\n",
    "                    'years_exp': years_exp,\n",
    "                    'skills': skills,\n",
    "                    'entities': entities\n",
    "                },\n",
    "                'file_size': os.path.getsize(file_path),\n",
    "                'text_length': len(text)\n",
    "            }\n",
    "            \n",
    "            processed_data.append(cv_data)\n",
    "            \n",
    "            # Print extracted information\n",
    "            print(f\"     Name: {cv_data['extracted_data']['name'] or 'Not found'}\")\n",
    "            print(f\"     Email: {cv_data['extracted_data']['email'] or 'Not found'}\")\n",
    "            print(f\"     Phone: {cv_data['extracted_data']['phone'] or 'Not found'}\")\n",
    "            print(f\"  üíº Years Exp: {cv_data['extracted_data']['years_exp']}\")\n",
    "            print(f\"  üõ†Ô∏è Skills: {len(skills)} skills found\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully processed {len(processed_data)}/{len(file_paths)} files\")\n",
    "    return processed_data\n",
    "\n",
    "# Process dataset\n",
    "if dataset_files:\n",
    "    processed_cvs = process_dataset_files(dataset_files)\n",
    "else:\n",
    "    print(\"‚ùå No files to process\")\n",
    "    processed_cvs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_statistics(processed_cvs: List[Dict]) -> Dict:\n",
    "    \"\"\"Ph√¢n t√≠ch th·ªëng k√™ dataset\"\"\"\n",
    "    if not processed_cvs:\n",
    "        return {}\n",
    "    \n",
    "    stats = {\n",
    "        'total_files': len(processed_cvs),\n",
    "        'file_types': {},\n",
    "        'text_lengths': [],\n",
    "        'file_sizes': [],\n",
    "        'extraction_success': {\n",
    "            'name': 0,\n",
    "            'email': 0,\n",
    "            'phone': 0,\n",
    "            'years_exp': 0,\n",
    "            'skills': 0\n",
    "        },\n",
    "        'skills_distribution': {},\n",
    "        'years_exp_distribution': []\n",
    "    }\n",
    "    \n",
    "    for cv in processed_cvs:\n",
    "        # File types\n",
    "        ext = os.path.splitext(cv['filename'])[1].lower()\n",
    "        stats['file_types'][ext] = stats['file_types'].get(ext, 0) + 1\n",
    "        \n",
    "        # Text lengths and file sizes\n",
    "        stats['text_lengths'].append(cv['text_length'])\n",
    "        stats['file_sizes'].append(cv['file_size'] / 1024)  # KB\n",
    "        \n",
    "        # Extraction success\n",
    "        extracted = cv['extracted_data']\n",
    "        if extracted['name']:\n",
    "            stats['extraction_success']['name'] += 1\n",
    "        if extracted['email']:\n",
    "            stats['extraction_success']['email'] += 1\n",
    "        if extracted['phone']:\n",
    "            stats['extraction_success']['phone'] += 1\n",
    "        if extracted['years_exp'] > 0:\n",
    "            stats['extraction_success']['years_exp'] += 1\n",
    "        if extracted['skills']:\n",
    "            stats['extraction_success']['skills'] += 1\n",
    "        \n",
    "        # Skills distribution\n",
    "        for skill in extracted['skills']:\n",
    "            stats['skills_distribution'][skill] = stats['skills_distribution'].get(skill, 0) + 1\n",
    "        \n",
    "        # Years experience distribution\n",
    "        if extracted['years_exp'] > 0:\n",
    "            stats['years_exp_distribution'].append(extracted['years_exp'])\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze dataset\n",
    "dataset_stats = analyze_dataset_statistics(processed_cvs)\n",
    "\n",
    "if dataset_stats:\n",
    "    print(\"\\nüìä Dataset Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total files: {dataset_stats['total_files']}\")\n",
    "    print(f\"\\nFile types:\")\n",
    "    for ext, count in dataset_stats['file_types'].items():\n",
    "        print(f\"  {ext}: {count} files\")\n",
    "    \n",
    "    print(f\"\\nText extraction:\")\n",
    "    print(f\"  Average text length: {np.mean(dataset_stats['text_lengths']):.0f} characters\")\n",
    "    print(f\"  Average file size: {np.mean(dataset_stats['file_sizes']):.1f} KB\")\n",
    "    \n",
    "    print(f\"\\nEntity extraction success rate:\")\n",
    "    total = dataset_stats['total_files']\n",
    "    for entity, count in dataset_stats['extraction_success'].items():\n",
    "        rate = (count / total) * 100 if total > 0 else 0\n",
    "        print(f\"  {entity}: {count}/{total} ({rate:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTop 10 skills:\")\n",
    "    top_skills = sorted(dataset_stats['skills_distribution'].items(), \n",
    "                       key=lambda x: x[1], reverse=True)[:10]\n",
    "    for skill, count in top_skills:\n",
    "        print(f\"  {skill}: {count} CVs\")\n",
    "    \n",
    "    if dataset_stats['years_exp_distribution']:\n",
    "        print(f\"\\nYears experience:\")\n",
    "        print(f\"  Average: {np.mean(dataset_stats['years_exp_distribution']):.1f} years\")\n",
    "        print(f\"  Range: {min(dataset_stats['years_exp_distribution'])} - {max(dataset_stats['years_exp_distribution'])} years\")\n",
    "else:\n",
    "    print(\"‚ùå No data to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Ranking with Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ranking_with_job_description(processed_cvs: List[Dict], job_description: str) -> List[Dict]:\n",
    "    \"\"\"Test ranking v·ªõi job description\"\"\"\n",
    "    if not processed_cvs:\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nüîç Testing ranking with job description...\")\n",
    "    print(f\"Job Description: {job_description[:100]}...\")\n",
    "    \n",
    "    # Extract CV texts\n",
    "    cv_texts = [cv['text'] for cv in processed_cvs]\n",
    "    \n",
    "    # Calculate semantic similarity\n",
    "    similarities = calculate_semantic_similarity(cv_texts, job_description)\n",
    "    \n",
    "    # Create ranking results\n",
    "    ranking_results = []\n",
    "    for i, cv in enumerate(processed_cvs):\n",
    "        ranking_results.append({\n",
    "            'filename': cv['filename'],\n",
    "            'name': cv['extracted_data']['name'],\n",
    "            'similarity_score': similarities[i],\n",
    "            'years_exp': cv['extracted_data']['years_exp'],\n",
    "            'skills': cv['extracted_data']['skills'],\n",
    "            'email': cv['extracted_data']['email']\n",
    "        })\n",
    "    \n",
    "    # Sort by similarity score\n",
    "    ranking_results.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "    \n",
    "    print(f\"\\n   Ranking Results:\")\n",
    "    for i, result in enumerate(ranking_results, 1):\n",
    "        print(f\"  {i}. {result['filename']} - {result['name']} (Score: {result['similarity_score']:.3f})\")\n",
    "    \n",
    "    return ranking_results\n",
    "\n",
    "# Test ranking v·ªõi job description m·∫´u\n",
    "sample_job_description = \"\"\"\n",
    "We are looking for a Senior Software Engineer with the following requirements:\n",
    "- 5+ years of software development experience\n",
    "- Strong programming skills in Python, Java, or JavaScript\n",
    "- Experience with web frameworks (Django, Flask, Spring, React)\n",
    "- Knowledge of databases (SQL, PostgreSQL, MongoDB)\n",
    "- Experience with cloud platforms (AWS, Azure, GCP)\n",
    "- Strong problem-solving and communication skills\n",
    "- Bachelor's degree in Computer Science or related field\n",
    "- Experience with version control (Git)\n",
    "- Knowledge of software development best practices\n",
    "\"\"\"\n",
    "\n",
    "if processed_cvs:\n",
    "    ranking_results = test_ranking_with_job_description(processed_cvs, sample_job_description)\n",
    "else:\n",
    "    print(\"‚ùå No CVs to rank\")\n",
    "    ranking_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_visualizations(dataset_stats: Dict, ranking_results: List[Dict]):\n",
    "    \"\"\"T·∫°o visualizations cho dataset analysis\"\"\"\n",
    "    if not dataset_stats:\n",
    "        print(\"‚ùå No data to visualize\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. File Types Distribution\n",
    "    file_types = list(dataset_stats['file_types'].keys())\n",
    "    file_counts = list(dataset_stats['file_types'].values())\n",
    "    \n",
    "    axes[0,0].pie(file_counts, labels=file_types, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0,0].set_title('File Types Distribution')\n",
    "    \n",
    "    # 2. Entity Extraction Success Rate\n",
    "    entities = list(dataset_stats['extraction_success'].keys())\n",
    "    success_counts = list(dataset_stats['extraction_success'].values())\n",
    "    total = dataset_stats['total_files']\n",
    "    success_rates = [(count / total) * 100 for count in success_counts]\n",
    "    \n",
    "    bars = axes[0,1].bar(entities, success_rates, color='skyblue', alpha=0.8)\n",
    "    axes[0,1].set_title('Entity Extraction Success Rate (%)')\n",
    "    axes[0,1].set_ylabel('Success Rate (%)')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].set_ylim(0, 100)\n",
    "    \n",
    "    # Th√™m gi√° tr·ªã l√™n bars\n",
    "    for bar, rate in zip(bars, success_rates):\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                       f'{rate:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Text Length Distribution\n",
    "    axes[0,2].hist(dataset_stats['text_lengths'], bins=20, color='lightgreen', alpha=0.7)\n",
    "    axes[0,2].set_title('Text Length Distribution')\n",
    "    axes[0,2].set_xlabel('Text Length (characters)')\n",
    "    axes[0,2].set_ylabel('Frequency')\n",
    "    \n",
    "    # 4. Top Skills Distribution\n",
    "    top_skills = sorted(dataset_stats['skills_distribution'].items(), \n",
    "                       key=lambda x: x[1], reverse=True)[:10]\n",
    "    if top_skills:\n",
    "        skills, counts = zip(*top_skills)\n",
    "        axes[1,0].barh(skills, counts, color='salmon', alpha=0.8)\n",
    "        axes[1,0].set_title('Top 10 Skills Distribution')\n",
    "        axes[1,0].set_xlabel('Number of CVs')\n",
    "    \n",
    "    # 5. Years Experience Distribution\n",
    "    if dataset_stats['years_exp_distribution']:\n",
    "        axes[1,1].hist(dataset_stats['years_exp_distribution'], bins=15, color='gold', alpha=0.7)\n",
    "        axes[1,1].set_title('Years Experience Distribution')\n",
    "        axes[1,1].set_xlabel('Years of Experience')\n",
    "        axes[1,1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 6. Ranking Results (if available)\n",
    "    if ranking_results:\n",
    "        filenames = [r['filename'][:15] + '...' if len(r['filename']) > 15 else r['filename'] \n",
    "                    for r in ranking_results[:10]]\n",
    "        scores = [r['similarity_score'] for r in ranking_results[:10]]\n",
    "        \n",
    "        bars = axes[1,2].bar(range(len(filenames)), scores, color='lightcoral', alpha=0.8)\n",
    "        axes[1,2].set_title('Top 10 CVs by Similarity Score')\n",
    "        axes[1,2].set_ylabel('Similarity Score')\n",
    "        axes[1,2].set_xticks(range(len(filenames)))\n",
    "        axes[1,2].set_xticklabels(filenames, rotation=45, ha='right')\n",
    "        \n",
    "        # Th√™m gi√° tr·ªã l√™n bars\n",
    "        for bar, score in zip(bars, scores):\n",
    "            axes[1,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                           f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Visualizations created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o visualizations\n",
    "create_dataset_visualizations(dataset_stats, ranking_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(processed_cvs: List[Dict], dataset_stats: Dict, ranking_results: List[Dict]):\n",
    "    \"\"\"Export k·∫øt qu·∫£ ph√¢n t√≠ch\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    \n",
    "    # T·∫°o summary report\n",
    "    summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'dataset_path': dataset_path,\n",
    "        'total_files_processed': len(processed_cvs),\n",
    "        'dataset_statistics': dataset_stats,\n",
    "        'ranking_results': ranking_results,\n",
    "        'extraction_summary': {\n",
    "            'successful_extractions': len(processed_cvs),\n",
    "            'name_extraction_rate': (dataset_stats['extraction_success']['name'] / len(processed_cvs)) * 100 if processed_cvs else 0,\n",
    "            'email_extraction_rate': (dataset_stats['extraction_success']['email'] / len(processed_cvs)) * 100 if processed_cvs else 0,\n",
    "            'phone_extraction_rate': (dataset_stats['extraction_success']['phone'] / len(processed_cvs)) * 100 if processed_cvs else 0,\n",
    "            'skills_extraction_rate': (dataset_stats['extraction_success']['skills'] / len(processed_cvs)) * 100 if processed_cvs else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # L∆∞u summary\n",
    "    with open('dataset_analysis_summary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # T·∫°o detailed CSV\n",
    "    if processed_cvs:\n",
    "        csv_data = []\n",
    "        for cv in processed_cvs:\n",
    "            csv_data.append({\n",
    "                'filename': cv['filename'],\n",
    "                'name': cv['extracted_data']['name'],\n",
    "                'email': cv['extracted_data']['email'],\n",
    "                'phone': cv['extracted_data']['phone'],\n",
    "                'years_exp': cv['extracted_data']['years_exp'],\n",
    "                'skills_count': len(cv['extracted_data']['skills']),\n",
    "                'skills': ', '.join(cv['extracted_data']['skills']),\n",
    "                'text_length': cv['text_length'],\n",
    "                'file_size_kb': cv['file_size'] / 1024\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(csv_data)\n",
    "        df.to_csv('dataset_analysis_detailed.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    # T·∫°o ranking CSV\n",
    "    if ranking_results:\n",
    "        ranking_df = pd.DataFrame(ranking_results)\n",
    "        ranking_df.to_csv('dataset_ranking_results.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    print(\"\\n‚úÖ Results exported:\")\n",
    "    print(\"  üìÑ dataset_analysis_summary.json - Summary report\")\n",
    "    if processed_cvs:\n",
    "        print(\"  üìä dataset_analysis_detailed.csv - Detailed CV data\")\n",
    "    if ranking_results:\n",
    "        print(\"  üìà dataset_ranking_results.csv - Ranking results\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Export results\n",
    "if processed_cvs:\n",
    "    export_summary = export_results(processed_cvs, dataset_stats, ranking_results)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ DATASET ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìÅ Dataset: {dataset_path}\")\n",
    "    print(f\"üìÑ Files processed: {len(processed_cvs)}\")\n",
    "    print(f\"\\n   Extraction Success Rates:\")\n",
    "    print(f\"  Name: {export_summary['extraction_summary']['name_extraction_rate']:.1f}%\")\n",
    "    print(f\"  Email: {export_summary['extraction_summary']['email_extraction_rate']:.1f}%\")\n",
    "    print(f\"  Phone: {export_summary['extraction_summary']['phone_extraction_rate']:.1f}%\")\n",
    "    print(f\"  Skills: {export_summary['extraction_summary']['skills_extraction_rate']:.1f}%\")\n",
    "    \n",
    "    if ranking_results:\n",
    "        print(f\"\\nüìà Ranking Results:\")\n",
    "        print(f\"  Top CV: {ranking_results[0]['filename']} (Score: {ranking_results[0]['similarity_score']:.3f})\")\n",
    "        print(f\"  Average similarity: {np.mean([r['similarity_score'] for r in ranking_results]):.3f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Analysis completed successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive analysis - cho ph√©p user test v·ªõi job description kh√°c\n",
    "def interactive_ranking_test(processed_cvs: List[Dict]):\n",
    "    \"\"\"Interactive ranking test\"\"\"\n",
    "    if not processed_cvs:\n",
    "        print(\"‚ùå No CVs to test\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n   Interactive Ranking Test\")\n",
    "    print(\"Enter a job description to test ranking (or 'quit' to exit):\")\n",
    "    \n",
    "    while True:\n",
    "        job_desc = input(\"\\nJob Description: \").strip()\n",
    "        \n",
    "        if job_desc.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        if not job_desc:\n",
    "            print(\"Please enter a job description\")\n",
    "            continue\n",
    "        \n",
    "        # Test ranking\n",
    "        results = test_ranking_with_job_description(processed_cvs, job_desc)\n",
    "        \n",
    "        # Show top 5 results\n",
    "        print(\"\\nüèÜ Top 5 Matches:\")\n",
    "        for i, result in enumerate(results[:5], 1):\n",
    "            print(f\"  {i}. {result['name']} - {result['filename']} (Score: {result['similarity_score']:.3f})\")\n",
    "            print(f\"     Experience: {result['years_exp']} years, Skills: {len(result['skills'])} skills\")\n",
    "\n",
    "# Uncomment ƒë·ªÉ ch·∫°y interactive test\n",
    "# interactive_ranking_test(processed_cvs)\n",
    "\n",
    "print(\"\\n   Dataset evaluation completed!\")\n",
    "print(\"\\nüìã Next steps:\")\n",
    "print(\"  1. Review the exported files\")\n",
    "print(\"  2. Analyze the visualizations\")\n",
    "print(\"  3. Use the ranking results for candidate selection\")\n",
    "print(\"  4. Run interactive ranking test if needed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}