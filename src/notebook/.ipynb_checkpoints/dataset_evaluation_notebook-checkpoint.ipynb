{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Resume Ranker - Dataset Evaluation Notebook\n",
    "\n",
    "Notebook này đọc folder chứa các file PDF/DOC, gọi model của web app để phân tích và đánh giá hiệu suất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thêm path để import modules\n",
    "sys.path.append('.')\n",
    "\n",
    "print(\"✅ Import libraries completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các modules từ web app\n",
    "try:\n",
    "    from services import (\n",
    "        extract_text,\n",
    "        extract_entities_ner,\n",
    "        extract_basic_entities,\n",
    "        extract_skills_spacy,\n",
    "        calculate_semantic_similarity,\n",
    "        nlp,\n",
    "        sbert_model\n",
    "    )\n",
    "    print(\"✅ Services imported successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error importing services: {e}\")\n",
    "\n",
    "try:\n",
    "    from control.uploads_controller import extract_cv_data\n",
    "    print(\"✅ Uploads controller imported successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error importing uploads controller: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thiết lập đường dẫn dataset\n",
    "dataset_path = input(\"Enter path to dataset folder: \").strip()\n",
    "if not dataset_path:\n",
    "    dataset_path = \"./dataset\"  # Default path\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"❌ Dataset folder not found: {dataset_path}\")\n",
    "    print(\"Please create the folder and add your PDF/DOC files\")\n",
    "else:\n",
    "    print(f\"✅ Dataset folder found: {dataset_path}\")\n",
    "\n",
    "# Liệt kê các file trong dataset\n",
    "def list_dataset_files(folder_path: str) -> List[str]:\n",
    "    \"\"\"Liệt kê tất cả file PDF và DOC trong folder\"\"\"\n",
    "    files = []\n",
    "    supported_extensions = ['.pdf', '.docx', '.doc']\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            _, ext = os.path.splitext(filename.lower())\n",
    "            if ext in supported_extensions:\n",
    "                files.append(file_path)\n",
    "    \n",
    "    return files\n",
    "\n",
    "dataset_files = list_dataset_files(dataset_path)\n",
    "print(f\"\\n   Found {len(dataset_files)} files in dataset:\")\n",
    "for i, file_path in enumerate(dataset_files, 1):\n",
    "    filename = os.path.basename(file_path)\n",
    "    file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "    print(f\"  {i}. {filename} ({file_size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_files(file_paths: List[str]) -> List[Dict]:\n",
    "    \"\"\"Xử lý tất cả file trong dataset\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    print(f\"\\n🔄 Processing {len(file_paths)} files...\")\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths, 1):\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"\\n📄 Processing {i}/{len(file_paths)}: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract text từ file\n",
    "            text = extract_text(file_path)\n",
    "            \n",
    "            if not text.strip():\n",
    "                print(f\"  ⚠️ No text extracted from {filename}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  ✅ Extracted {len(text)} characters\")\n",
    "            \n",
    "            # Extract entities using web app functions\n",
    "            entities = extract_entities_ner(text)\n",
    "            emails, phones, years_exp = extract_basic_entities(text)\n",
    "            skills = extract_skills_spacy(text)\n",
    "            \n",
    "            # Tạo CV data structure\n",
    "            cv_data = {\n",
    "                'filename': filename,\n",
    "                'file_path': file_path,\n",
    "                'text': text,\n",
    "                'extracted_data': {\n",
    "                    'name': entities['PERSON'][0] if entities['PERSON'] else \"\",\n",
    "                    'email': emails[0] if emails else \"\",\n",
    "                    'phone': phones[0] if phones else \"\",\n",
    "                    'years_exp': years_exp,\n",
    "                    'skills': skills,\n",
    "                    'entities': entities\n",
    "                },\n",
    "                'file_size': os.path.getsize(file_path),\n",
    "                'text_length': len(text)\n",
    "            }\n",
    "            \n",
    "            processed_data.append(cv_data)\n",
    "            \n",
    "            # Print extracted information\n",
    "            print(f\"     Name: {cv_data['extracted_data']['name'] or 'Not found'}\")\n",
    "            print(f\"     Email: {cv_data['extracted_data']['email'] or 'Not found'}\")\n",
    "            print(f\"     Phone: {cv_data['extracted_data']['phone'] or 'Not found'}\")\n",
    "            print(f\"  💼 Years Exp: {cv_data['extracted_data']['years_exp']}\")\n",
    "            print(f\"  🛠️ Skills: {len(skills)} skills found\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✅ Successfully processed {len(processed_data)}/{len(file_paths)} files\")\n",
    "    return processed_data\n",
    "\n",
    "# Process dataset\n",
    "if dataset_files:\n",
    "    processed_cvs = process_dataset_files(dataset_files)\n",
    "else:\n",
    "    print(\"❌ No files to process\")\n",
    "    processed_cvs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_statistics(processed_cvs: List[Dict]) -> Dict:\n",
    "    \"\"\"Phân tích thống kê dataset\"\"\"\n",
    "    if not processed_cvs:\n",
    "        return {}\n",
    "    \n",
    "    stats = {\n",
    "        'total_files': len(processed_cvs),\n",
    "        'file_types': {},\n",
    "        'text_lengths': [],\n",
    "        'file_sizes': [],\n",
    "        'extraction_success': {\n",
    "            'name': 0,\n",
    "            'email': 0,\n",
    "            'phone': 0,\n",
    "            'years_exp': 0,\n",
    "            'skills': 0\n",
    "        },\n",
    "        'skills_distribution': {},\n",
    "        'years_exp_distribution': []\n",
    "    }\n",
    "    \n",
    "    for cv in processed_cvs:\n",
    "        # File types\n",
    "        ext = os.path.splitext(cv['filename'])[1].lower()\n",
    "        stats['file_types'][ext] = stats['file_types'].get(ext, 0) + 1\n",
    "        \n",
    "        # Text lengths and file sizes\n",
    "        stats['text_lengths'].append(cv['text_length'])\n",
    "        stats['file_sizes'].append(cv['file_size'] / 1024)  # KB\n",
    "        \n",
    "        # Extraction success\n",
    "        extracted = cv['extracted_data']\n",
    "        if extracted['name']:\n",
    "            stats['extraction_success']['name'] += 1\n",
    "        if extracted['email']:\n",
    "            stats['extraction_success']['email'] += 1\n",
    "        if extracted['phone']:\n",
    "            stats['extraction_success']['phone'] += 1\n",
    "        if extracted['years_exp'] > 0:\n",
    "            stats['extraction_success']['years_exp'] += 1\n",
    "        if extracted['skills']:\n",
    "            stats['extraction_success']['skills'] += 1\n",
    "        \n",
    "        # Skills distribution\n",
    "        for skill in extracted['skills']:\n",
    "            stats['skills_distribution'][skill] = stats['skills_distribution'].get(skill, 0) + 1\n",
    "        \n",
    "        # Years experience distribution\n",
    "        if extracted['years_exp'] > 0:\n",
    "            stats['years_exp_distribution'].append(extracted['years_exp'])\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analyze dataset\n",
    "dataset_stats = analyze_dataset_statistics(processed_cvs)\n",
    "\n",
    "if dataset_stats:\n",
    "    print(\"\\n📊 Dataset Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total files: {dataset_stats['total_files']}\")\n",
    "    print(f\"\\nFile types:\")\n",
    "    for ext, count in dataset_stats['file_types'].items():\n",
    "        print(f\"  {ext}: {count} files\")\n",
    "    \n",
    "    print(f\"\\nText extraction:\")\n",
    "    print(f\"  Average text length: {np.mean(dataset_stats['text_lengths']):.0f} characters\")\n",
    "    print(f\"  Average file size: {np.mean(dataset_stats['file_sizes']):.1f} KB\")\n",
    "    \n",
    "    print(f\"\\nEntity extraction success rate:\")\n",
    "    total = dataset_stats['total_files']\n",
    "    for entity, count in dataset_stats['extraction_success'].items():\n",
    "        rate = (count / total) * 100 if total > 0 else 0\n",
    "        print(f\"  {entity}: {count}/{total} ({rate:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTop 10 skills:\")\n",
    "    top_skills = sorted(dataset_stats['skills_distribution'].items(), \n",
    "                       key=lambda x: x[1], reverse=True)[:10]\n",
    "    for skill, count in top_skills:\n",
    "        print(f\"  {skill}: {count} CVs\")\n",
    "    \n",
    "    if dataset_stats['years_exp_distribution']:\n",
    "        print(f\"\\nYears experience:\")\n",
    "        print(f\"  Average: {np.mean(dataset_stats['years_exp_distribution']):.1f} years\")\n",
    "        print(f\"  Range: {min(dataset_stats['years_exp_distribution'])} - {max(dataset_stats['years_exp_distribution'])} years\")\n",
    "else:\n",
    "    print(\"❌ No data to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Ranking with Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ranking_with_job_description(processed_cvs: List[Dict], job_description: str) -> List[Dict]:\n",
    "    \"\"\"Test ranking với job description\"\"\"\n",
    "    if not processed_cvs:\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\n🔍 Testing ranking with job description...\")\n",
    "    print(f\"Job Description: {job_description[:100]}...\")\n",
    "    \n",
    "    # Extract CV texts\n",
    "    cv_texts = [cv['text'] for cv in processed_cvs]\n",
    "    \n",
    "    # Calculate semantic similarity\n",
    "    similarities = calculate_semantic_similarity(cv_texts, job_description)\n",
    "    \n",
    "    # Create ranking results\n",
    "    ranking_results = []\n",
    "    for i, cv in enumerate(processed_cvs):\n",
    "        ranking_results.append({\n",
    "            'filename': cv['filename'],\n",
    "            'name': cv['extracted_data']['name'],\n",
    "            'similarity_score': similarities[i],\n",
    "            'years_exp': cv['extracted_data']['years_exp'],\n",
    "            'skills': cv['extracted_data']['skills'],\n",
    "            'email': cv['extracted_data']['email']\n",
    "        })\n",
    "    \n",
    "    # Sort by similarity score\n",
    "    ranking_results.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "    \n",
    "    print(f\"\\n   Ranking Results:\")\n",
    "    for i, result in enumerate(ranking_results, 1):\n",
    "        print(f\"  {i}. {result['filename']} - {result['name']} (Score: {result['similarity_score']:.3f})\")\n",
    "    \n",
    "    return ranking_results\n",
    "\n",
    "# Test ranking với job description mẫu\n",
    "sample_job_description = \"\"\"\n",
    "We are looking for a Senior Software Engineer with the following requirements:\n",
    "- 5+ years of software development experience\n",
    "- Strong programming skills in Python, Java, or JavaScript\n",
    "- Experience with web frameworks (Django, Flask, Spring, React)\n",
    "- Knowledge of databases (SQL, PostgreSQL, MongoDB)\n",
    "- Experience with cloud platforms (AWS, Azure, GCP)\n",
    "- Strong problem-solving and communication skills\n",
    "- Bachelor's degree in Computer Science or related field\n",
    "- Experience with version control (Git)\n",
    "- Knowledge of software development best practices\n",
    "\"\"\"\n",
    "\n",
    "if processed_cvs:\n",
    "    ranking_results = test_ranking_with_job_description(processed_cvs, sample_job_description)\n",
    "else:\n",
    "    print(\"❌ No CVs to rank\")\n",
    "    ranking_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_visualizations(dataset_stats: Dict, ranking_results: List[Dict]):\n",
    "    \"\"\"Tạo visualizations cho dataset analysis\"\"\"\n",
    "    if not dataset_stats:\n",
    "        print(\"❌ No data to visualize\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. File Types Distribution\n",
    "    file_types = list(dataset_stats['file_types'].keys())\n",
    "    file_counts = list(dataset_stats['file_types'].values())\n",
    "    \n",
    "    axes[0,0].pie(file_counts, labels=file_types, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0,0].set_title('File Types Distribution')\n",
    "    \n",
    "    # 2. Entity Extraction Success Rate\n",
    "    entities = list(dataset_stats['extraction_success'].keys())\n",
    "    success_counts = list(dataset_stats['extraction_success'].values())\n",
    "    total = dataset_stats['total_files']\n",
    "    success_rates = [(count / total) * 100 for count in success_counts]\n",
    "    \n",
    "    bars = axes[0,1].bar(entities, success_rates, color='skyblue', alpha=0.8)\n",
    "    axes[0,1].set_title('Entity Extraction Success Rate (%)')\n",
    "    axes[0,1].set_ylabel('Success Rate (%)')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].set_ylim(0, 100)\n",
    "    \n",
    "    # Thêm giá trị lên bars\n",
    "    for bar, rate in zip(bars, success_rates):\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                       f'{rate:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Text Length Distribution\n",
    "    axes[0,2].hist(dataset_stats['text_lengths'], bins=20, color='lightgreen', alpha=0.7)\n",
    "    axes[0,2].set_title('Text Length Distribution')\n",
    "    axes[0,2].set_xlabel('Text Length (characters)')\n",
    "    axes[0,2].set_ylabel('Frequency')\n",
    "    \n",
    "    # 4. Top Skills Distribution\n",
    "    top_skills = sorted(dataset_stats['skills_distribution'].items(), \n",
    "                       key=lambda x: x[1], reverse=True)[:10]\n",
    "    if top_skills:\n",
    "        skills, counts = zip(*top_skills)\n",
    "        axes[1,0].barh(skills, counts, color='salmon', alpha=0.8)\n",
    "        axes[1,0].set_title('Top 10 Skills Distribution')\n",
    "        axes[1,0].set_xlabel('Number of CVs')\n",
    "    \n",
    "    # 5. Years Experience Distribution\n",
    "    if dataset_stats['years_exp_distribution']:\n",
    "        axes[1,1].hist(dataset_stats['years_exp_distribution'], bins=15, color='gold', alpha=0.7)\n",
    "        axes[1,1].set_title('Years Experience Distribution')\n",
    "        axes[1,1].set_xlabel('Years of Experience')\n",
    "        axes[1,1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 6. Ranking Results (if available)\n",
    "    if ranking_results:\n",
    "        filenames = [r['filename'][:15] + '...' if len(r['filename']) > 15 else r['filename'] \n",
    "                    for r in ranking_results[:10]]\n",
    "        scores = [r['similarity_score'] for r in ranking_results[:10]]\n",
    "        \n",
    "        bars = axes[1,2].bar(range(len(filenames)), scores, color='lightcoral', alpha=0.8)\n",
    "        axes[1,2].set_title('Top 10 CVs by Similarity Score')\n",
    "        axes[1,2].set_ylabel('Similarity Score')\n",
    "        axes[1,2].set_xticks(range(len(filenames)))\n",
    "        axes[1,2].set_xticklabels(filenames, rotation=45, ha='right')\n",
    "        \n",
    "        # Thêm giá trị lên bars\n",
    "        for bar, score in zip(bars, scores):\n",
    "            axes[1,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                           f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n📊 Visualizations created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo visualizations\n",
    "create_dataset_visualizations(dataset_stats, ranking_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(processed_cvs: List[Dict], dataset_stats: Dict, ranking_results: List[Dict]):\n",
    "    \"\"\"Export kết quả phân tích\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    \n",
    "    # Tạo summary report\n",
    "    summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'dataset_path': dataset_path,\n",
    "        'total_files_processed': len(processed_cvs),\n",
    "        'dataset_statistics': dataset_stats,\n",
    "        'ranking_results': ranking_results,\n",
    "        'extraction_summary': {\n",
    "            'successful_extractions': len(processed_cvs),\n",
    "            'name_extraction_rate': (dataset_stats['extraction_success']['name'] / len(processed_cvs)) * 100 if processed_cvs else 0,\n",
    "            'email_extraction_rate': (dataset_stats['extraction_success']['email'] / len(processed_cvs)) * 100 if processed_cvs else 0,\n",
    "            'phone_extraction_rate': (dataset_stats['extraction_success']['phone'] / len(processed_cvs)) * 100 if processed_cvs else 0,\n",
    "            'skills_extraction_rate': (dataset_stats['extraction_success']['skills'] / len(processed_cvs)) * 100 if processed_cvs else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Lưu summary\n",
    "    with open('dataset_analysis_summary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Tạo detailed CSV\n",
    "    if processed_cvs:\n",
    "        csv_data = []\n",
    "        for cv in processed_cvs:\n",
    "            csv_data.append({\n",
    "                'filename': cv['filename'],\n",
    "                'name': cv['extracted_data']['name'],\n",
    "                'email': cv['extracted_data']['email'],\n",
    "                'phone': cv['extracted_data']['phone'],\n",
    "                'years_exp': cv['extracted_data']['years_exp'],\n",
    "                'skills_count': len(cv['extracted_data']['skills']),\n",
    "                'skills': ', '.join(cv['extracted_data']['skills']),\n",
    "                'text_length': cv['text_length'],\n",
    "                'file_size_kb': cv['file_size'] / 1024\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(csv_data)\n",
    "        df.to_csv('dataset_analysis_detailed.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    # Tạo ranking CSV\n",
    "    if ranking_results:\n",
    "        ranking_df = pd.DataFrame(ranking_results)\n",
    "        ranking_df.to_csv('dataset_ranking_results.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    print(\"\\n✅ Results exported:\")\n",
    "    print(\"  📄 dataset_analysis_summary.json - Summary report\")\n",
    "    if processed_cvs:\n",
    "        print(\"  📊 dataset_analysis_detailed.csv - Detailed CV data\")\n",
    "    if ranking_results:\n",
    "        print(\"  📈 dataset_ranking_results.csv - Ranking results\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Export results\n",
    "if processed_cvs:\n",
    "    export_summary = export_results(processed_cvs, dataset_stats, ranking_results)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 DATASET ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"📁 Dataset: {dataset_path}\")\n",
    "    print(f\"📄 Files processed: {len(processed_cvs)}\")\n",
    "    print(f\"\\n   Extraction Success Rates:\")\n",
    "    print(f\"  Name: {export_summary['extraction_summary']['name_extraction_rate']:.1f}%\")\n",
    "    print(f\"  Email: {export_summary['extraction_summary']['email_extraction_rate']:.1f}%\")\n",
    "    print(f\"  Phone: {export_summary['extraction_summary']['phone_extraction_rate']:.1f}%\")\n",
    "    print(f\"  Skills: {export_summary['extraction_summary']['skills_extraction_rate']:.1f}%\")\n",
    "    \n",
    "    if ranking_results:\n",
    "        print(f\"\\n📈 Ranking Results:\")\n",
    "        print(f\"  Top CV: {ranking_results[0]['filename']} (Score: {ranking_results[0]['similarity_score']:.3f})\")\n",
    "        print(f\"  Average similarity: {np.mean([r['similarity_score'] for r in ranking_results]):.3f}\")\n",
    "    \n",
    "    print(f\"\\n✅ Analysis completed successfully!\")\n",
    "else:\n",
    "    print(\"❌ No data to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive analysis - cho phép user test với job description khác\n",
    "def interactive_ranking_test(processed_cvs: List[Dict]):\n",
    "    \"\"\"Interactive ranking test\"\"\"\n",
    "    if not processed_cvs:\n",
    "        print(\"❌ No CVs to test\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n   Interactive Ranking Test\")\n",
    "    print(\"Enter a job description to test ranking (or 'quit' to exit):\")\n",
    "    \n",
    "    while True:\n",
    "        job_desc = input(\"\\nJob Description: \").strip()\n",
    "        \n",
    "        if job_desc.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        if not job_desc:\n",
    "            print(\"Please enter a job description\")\n",
    "            continue\n",
    "        \n",
    "        # Test ranking\n",
    "        results = test_ranking_with_job_description(processed_cvs, job_desc)\n",
    "        \n",
    "        # Show top 5 results\n",
    "        print(\"\\n🏆 Top 5 Matches:\")\n",
    "        for i, result in enumerate(results[:5], 1):\n",
    "            print(f\"  {i}. {result['name']} - {result['filename']} (Score: {result['similarity_score']:.3f})\")\n",
    "            print(f\"     Experience: {result['years_exp']} years, Skills: {len(result['skills'])} skills\")\n",
    "\n",
    "# Uncomment để chạy interactive test\n",
    "# interactive_ranking_test(processed_cvs)\n",
    "\n",
    "print(\"\\n   Dataset evaluation completed!\")\n",
    "print(\"\\n📋 Next steps:\")\n",
    "print(\"  1. Review the exported files\")\n",
    "print(\"  2. Analyze the visualizations\")\n",
    "print(\"  3. Use the ranking results for candidate selection\")\n",
    "print(\"  4. Run interactive ranking test if needed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}