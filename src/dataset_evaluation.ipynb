{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Resume Ranker - Dataset Evaluation Notebook\n",
        "\n",
        "This notebook reads folder containing PDF/DOC files, calls web app models to analyze and evaluate performance.\n",
        "\n",
        "## Features:\n",
        "- Parse PDF/DOC files into JSON format\n",
        "- Extract entities using NER models  \n",
        "- Calculate semantic similarity\n",
        "- Generate performance metrics\n",
        "- Create visualizations\n",
        "- Export results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Tuple\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add path to import modules\n",
        "sys.path.append('.')\n",
        "\n",
        "print(\"‚úÖ Import libraries completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import modules from web app\n",
        "try:\n",
        "    from services import (\n",
        "        extract_text, extract_entities_ner, extract_basic_entities, \n",
        "        extract_skills_spacy, calculate_semantic_similarity\n",
        "    )\n",
        "    print(\"‚úÖ Web app modules imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importing web app modules: {e}\")\n",
        "    print(\"Make sure you're running this notebook from the src directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup dataset path\n",
        "dataset_path = input(\"Enter path to dataset folder: \").strip()\n",
        "if not dataset_path:\n",
        "    dataset_path = \"notebook/dataset\"  # Default path\n",
        "\n",
        "print(f\"üìÅ Dataset path: {dataset_path}\")\n",
        "\n",
        "# Check if path exists\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(f\"‚ùå Dataset path does not exist: {dataset_path}\")\n",
        "    print(\"Please create the dataset folder and add your PDF/DOC files\")\n",
        "else:\n",
        "    print(\"‚úÖ Dataset path exists\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List files in dataset\n",
        "def list_dataset_files(dataset_path):\n",
        "    \"\"\"List all PDF and DOC files in folder\"\"\"\n",
        "    if not os.path.exists(dataset_path):\n",
        "        return []\n",
        "    \n",
        "    files = []\n",
        "    for file in os.listdir(dataset_path):\n",
        "        if file.lower().endswith(('.pdf', '.docx', '.doc')):\n",
        "            files.append(file)\n",
        "    \n",
        "    return sorted(files)\n",
        "\n",
        "# Get list of files\n",
        "dataset_files = list_dataset_files(dataset_path)\n",
        "print(f\"üìä Found {len(dataset_files)} files in dataset\")\n",
        "\n",
        "if dataset_files:\n",
        "    print(\"\\\\nFiles found:\")\n",
        "    for i, file in enumerate(dataset_files[:10], 1):  # Show first 10 files\n",
        "        print(f\"  {i}. {file}\")\n",
        "    \n",
        "    if len(dataset_files) > 10:\n",
        "        print(f\"  ... and {len(dataset_files) - 10} more files\")\n",
        "else:\n",
        "    print(\"‚ùå No PDF/DOC files found in dataset folder\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all files in dataset\n",
        "def process_dataset_files(dataset_path, dataset_files):\n",
        "    \"\"\"Process all files in dataset\"\"\"\n",
        "    processed_cvs = []\n",
        "    \n",
        "    print(f\"üîÑ Processing {len(dataset_files)} files...\")\n",
        "    \n",
        "    for i, filename in enumerate(dataset_files, 1):\n",
        "        print(f\"\\\\nProcessing {i}/{len(dataset_files)}: {filename}\")\n",
        "        \n",
        "        file_path = os.path.join(dataset_path, filename)\n",
        "        \n",
        "        try:\n",
        "            # Extract text from file\n",
        "            text = extract_text(file_path)\n",
        "            if not text:\n",
        "                print(f\"‚ùå Failed to extract text from {filename}\")\n",
        "                continue\n",
        "            \n",
        "            # Extract entities\n",
        "            entities = extract_entities_ner(text)\n",
        "            basic = extract_basic_entities(text)\n",
        "            skills = extract_skills_spacy(text)\n",
        "            \n",
        "            # Create CV data structure\n",
        "            cv_data = {\n",
        "                'filename': filename,\n",
        "                'text': text,\n",
        "                'entities': entities,\n",
        "                'name': basic.get('name', ''),\n",
        "                'email': basic.get('email', ''),\n",
        "                'phone': basic.get('phone', ''),\n",
        "                'years_exp': basic.get('years_exp', 0),\n",
        "                'skills': skills,\n",
        "                'processed_at': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            processed_cvs.append(cv_data)\n",
        "            print(f\"‚úÖ Processed: {filename}\")\n",
        "            print(f\"   - Text length: {len(text)} characters\")\n",
        "            print(f\"   - Skills found: {len(skills)}\")\n",
        "            print(f\"   - Years experience: {basic.get('years_exp', 0)}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return processed_cvs\n",
        "\n",
        "# Process dataset\n",
        "if dataset_files:\n",
        "    processed_cvs = process_dataset_files(dataset_path, dataset_files)\n",
        "    print(f\"\\\\nüìã Processing completed: {len(processed_cvs)} CVs processed successfully\")\n",
        "else:\n",
        "    print(\"‚ùå No files to process\")\n",
        "    processed_cvs = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Job Description input and parsing\n",
        "print(\"üìù Job Description Input\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get job description from user\n",
        "job_description = input(\"Enter job description (or press Enter to use sample): \").strip()\n",
        "\n",
        "if not job_description:\n",
        "    # Sample job description\n",
        "    job_description = \"\"\"\n",
        "    We are looking for a Senior Software Engineer with the following requirements:\n",
        "    \n",
        "    Required Skills:\n",
        "    - Python programming (5+ years)\n",
        "    - JavaScript/React (3+ years)\n",
        "    - SQL and database design\n",
        "    - AWS cloud services\n",
        "    - Docker and Kubernetes\n",
        "    - Git version control\n",
        "    \n",
        "    Experience:\n",
        "    - 5+ years of software development experience\n",
        "    - Experience with microservices architecture\n",
        "    - Experience with CI/CD pipelines\n",
        "    - Experience with Agile/Scrum methodologies\n",
        "    \n",
        "    Education:\n",
        "    - Bachelor's degree in Computer Science or related field\n",
        "    \n",
        "    Responsibilities:\n",
        "    - Design and develop scalable web applications\n",
        "    - Collaborate with cross-functional teams\n",
        "    - Mentor junior developers\n",
        "    - Participate in code reviews\n",
        "    - Contribute to technical documentation\n",
        "    \"\"\"\n",
        "    print(\"Using sample job description\")\n",
        "\n",
        "print(f\"\\\\nüìÑ Job Description length: {len(job_description)} characters\")\n",
        "print(f\"üìÑ Job Description preview: {job_description[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run web app analysis and ranking\n",
        "def run_web_app_analysis(processed_cvs, job_description):\n",
        "    \"\"\"Run analysis using web app models\"\"\"\n",
        "    if not processed_cvs or not job_description:\n",
        "        print(\"‚ùå No data available for analysis\")\n",
        "        return None\n",
        "    \n",
        "    print(\"üîÑ Running web app analysis...\")\n",
        "    \n",
        "    try:\n",
        "        # Get resume texts\n",
        "        resume_texts = [cv['text'] for cv in processed_cvs]\n",
        "        \n",
        "        # Calculate semantic similarity using web app function\n",
        "        similarities = calculate_semantic_similarity(resume_texts, job_description)\n",
        "        \n",
        "        # Handle both list and numpy array\n",
        "        if isinstance(similarities, list):\n",
        "            similarities = similarities\n",
        "        else:\n",
        "            similarities = similarities.tolist()\n",
        "        \n",
        "        # Create ranking results\n",
        "        ranking_results = []\n",
        "        for i, cv in enumerate(processed_cvs):\n",
        "            ranking_results.append({\n",
        "                'filename': cv['filename'],\n",
        "                'name': cv['name'],\n",
        "                'email': cv['email'],\n",
        "                'phone': cv['phone'],\n",
        "                'years_exp': cv['years_exp'],\n",
        "                'skills': cv['skills'],\n",
        "                'semantic_score': similarities[i] if i < len(similarities) else 0.0,\n",
        "                'rank': i + 1\n",
        "            })\n",
        "        \n",
        "        # Sort by semantic score\n",
        "        ranking_results.sort(key=lambda x: x['semantic_score'], reverse=True)\n",
        "        \n",
        "        # Update ranks\n",
        "        for i, result in enumerate(ranking_results):\n",
        "            result['rank'] = i + 1\n",
        "        \n",
        "        print(f\"‚úÖ Analysis completed: {len(ranking_results)} candidates ranked\")\n",
        "        return ranking_results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in analysis: {e}\")\n",
        "        return None\n",
        "\n",
        "# Run analysis\n",
        "if processed_cvs and job_description:\n",
        "    ranking_results = run_web_app_analysis(processed_cvs, job_description)\n",
        "else:\n",
        "    print(\"‚ùå No data available for analysis\")\n",
        "    ranking_results = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display ranking results\n",
        "if ranking_results:\n",
        "    print(\"üèÜ RANKING RESULTS:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Show top 10 candidates\n",
        "    for i, result in enumerate(ranking_results[:10], 1):\n",
        "        print(f\"\\\\n{i}. {result['name'] or result['filename']}\")\n",
        "        print(f\"   Email: {result['email']}\")\n",
        "        print(f\"   Phone: {result['phone']}\")\n",
        "        print(f\"   Experience: {result['years_exp']} years\")\n",
        "        print(f\"   Skills: {', '.join(result['skills'][:5])}{'...' if len(result['skills']) > 5 else ''}\")\n",
        "        print(f\"   Semantic Score: {result['semantic_score']:.3f}\")\n",
        "        print(f\"   Rank: {result['rank']}\")\n",
        "    \n",
        "    if len(ranking_results) > 10:\n",
        "        print(f\"\\\\n... and {len(ranking_results) - 10} more candidates\")\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(f\"\\\\nüìä SUMMARY STATISTICS:\")\n",
        "    print(f\"Total candidates: {len(ranking_results)}\")\n",
        "    print(f\"Average semantic score: {np.mean([r['semantic_score'] for r in ranking_results]):.3f}\")\n",
        "    print(f\"Highest score: {max([r['semantic_score'] for r in ranking_results]):.3f}\")\n",
        "    print(f\"Lowest score: {min([r['semantic_score'] for r in ranking_results]):.3f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No ranking results available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations\n",
        "def create_visualizations(ranking_results):\n",
        "    \"\"\"Create visualizations for dataset analysis\"\"\"\n",
        "    if not ranking_results:\n",
        "        print(\"‚ùå No data available for visualization\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('AI Resume Ranker - Dataset Analysis', fontsize=16)\n",
        "    \n",
        "    # 1. Semantic Scores\n",
        "    scores = [r['semantic_score'] for r in ranking_results[:10]]\n",
        "    names = [r['name'] or r['filename'][:20] for r in ranking_results[:10]]\n",
        "    \n",
        "    axes[0, 0].bar(range(len(scores)), scores, color='skyblue')\n",
        "    axes[0, 0].set_title('Top 10 Candidates - Semantic Scores')\n",
        "    axes[0, 0].set_xlabel('Candidates')\n",
        "    axes[0, 0].set_ylabel('Semantic Score')\n",
        "    axes[0, 0].set_xticks(range(len(names)))\n",
        "    axes[0, 0].set_xticklabels(names, rotation=45, ha='right')\n",
        "    \n",
        "    # Add values on bars\n",
        "    for i, v in enumerate(scores):\n",
        "        axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # 2. Years of Experience Distribution\n",
        "    exp_values = [r['years_exp'] for r in ranking_results]\n",
        "    axes[0, 1].hist(exp_values, bins=10, color='orange', alpha=0.7)\n",
        "    axes[0, 1].set_title('Years of Experience Distribution')\n",
        "    axes[0, 1].set_xlabel('Years of Experience')\n",
        "    axes[0, 1].set_ylabel('Number of Candidates')\n",
        "    \n",
        "    # 3. Skills Count Distribution\n",
        "    skills_counts = [len(r['skills']) for r in ranking_results]\n",
        "    axes[1, 0].hist(skills_counts, bins=10, color='green', alpha=0.7)\n",
        "    axes[1, 0].set_title('Skills Count Distribution')\n",
        "    axes[1, 0].set_xlabel('Number of Skills')\n",
        "    axes[1, 0].set_ylabel('Number of Candidates')\n",
        "    \n",
        "    # 4. Skills Count vs Semantic Score\n",
        "    axes[1, 1].scatter(skills_counts, [r['semantic_score'] for r in ranking_results], alpha=0.6, color='purple')\n",
        "    axes[1, 1].set_title('Skills Count vs Semantic Score')\n",
        "    axes[1, 1].set_xlabel('Number of Skills')\n",
        "    axes[1, 1].set_ylabel('Semantic Score')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create visualizations\n",
        "if ranking_results:\n",
        "    create_visualizations(ranking_results)\n",
        "else:\n",
        "    print(\"‚ùå No data available for visualization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export results\n",
        "def export_analysis_results(processed_cvs, ranking_results, job_description):\n",
        "    \"\"\"Export analysis results to files\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    \n",
        "    # Export processed CVs\n",
        "    with open(f'processed_cvs_{timestamp}.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(processed_cvs, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"‚úÖ Exported processed CVs: processed_cvs_{timestamp}.json\")\n",
        "    \n",
        "    # Export job description\n",
        "    jd_data = {'job_description': job_description, 'timestamp': timestamp}\n",
        "    with open(f'job_description_{timestamp}.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(jd_data, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"‚úÖ Exported job description: job_description_{timestamp}.json\")\n",
        "    \n",
        "    # Export ranking results as CSV\n",
        "    if ranking_results:\n",
        "        ranking_df = pd.DataFrame(ranking_results)\n",
        "        ranking_df.to_csv(f'ranking_results_{timestamp}.csv', index=False)\n",
        "        print(f\"‚úÖ Exported ranking results: ranking_results_{timestamp}.csv\")\n",
        "    \n",
        "    # Create summary report\n",
        "    summary = {\n",
        "        'timestamp': timestamp,\n",
        "        'total_cvs_processed': len(processed_cvs),\n",
        "        'total_candidates_ranked': len(ranking_results),\n",
        "        'job_description_length': len(job_description),\n",
        "        'top_candidate': ranking_results[0] if ranking_results else None,\n",
        "        'average_semantic_score': np.mean([r['semantic_score'] for r in ranking_results]) if ranking_results else 0\n",
        "    }\n",
        "    \n",
        "    with open(f'summary_report_{timestamp}.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"‚úÖ Exported summary report: summary_report_{timestamp}.json\")\n",
        "\n",
        "# Export results\n",
        "if processed_cvs and ranking_results:\n",
        "    export_analysis_results(processed_cvs, ranking_results, job_description)\n",
        "    print(\"\\\\nüéâ All results exported successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå No data available for export\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
